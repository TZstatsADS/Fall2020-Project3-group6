colsample_bytree=0.8, scale_pos_weight=4)
if(run.xgboost){
train_result_xgb <- xgb_train(dat_train, para)
model_xgb <- train_result_xgb[[1]]
train_time_xgb <- train_result_xgb[[2]]
save(model_xgb, file="../output/model_xgb.RData")
save(train_time_xgb, file="../output/train_time_xgb.RData")
}
if(run.xgboost){
source("../lib/test_xgboost.R")
load("../output/model_xgb.RData")
load("../output/feature_test.RData")
test_result_xgb = xgb_test(model_xgb, dat_test)
pred_xgb = test_result_xgb[[1]]
pred_xgb = ifelse(pred_xgb>0.5, 1, 0)
test_time_xgb = test_result_xgb[[2]]
accuracy_xgb = mean(dat_test$label == pred_xgb)
print(paste0("The accuracy for xgb model is: ", accuracy_xgb))
print(paste0("The training time of xgb model is:", train_time_xgb[3]))
print(paste0("The test time of xgb model is:", test_time_xgb[3]))
#confusionMatrix(table(pred_xgb, dat_test$label))
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_xgb[3], "s \n")
cat("Time for testing model=", test_time_xgb[3], "s \n")
tm_feature_train
train_time_xgb
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_xgb[1], "s \n")
cat("Time for testing model=", test_time_xgb[1], "s \n")
as.integer(dat_test$label)
if(run.xgboost){
source("../lib/test_xgboost.R")
load("../output/model_xgb.RData")
load("../output/feature_test.RData")
test_result_xgb = xgb_test(model_xgb, dat_test)
pred_xgb = test_result_xgb[[1]]
pred_xgb = ifelse(pred_xgb>0.5, 1, 0)
test_time_xgb = test_result_xgb[[2]]
accuracy_xgb = mean(dat_test$label == pred_xgb)
label_test <- as.numeric(dat_test$label)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_xgb, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
#print(paste0("The accuracy for xgb model is: ", accuracy_xgb))
#print(paste0("The training time of xgb model is:", train_time_xgb[3]))
#print(paste0("The test time of xgb model is:", test_time_xgb[3]))
#confusionMatrix(table(pred_xgb, dat_test$label))
}
## reweight the test data to represent a balanced label distribution
cat("The accuracy of xgb model is:", accuracy_xgb*100, "%.\n")
cat("The AUC of xgb model is:", auc, ".\n")
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_xgb[1], "s \n")
cat("Time for testing model=", test_time_xgb[1], "s \n")
# Forward Stepwise Selection-------------------------------------------------------------------------
forward_fit <- regsubsets(label ~ ., data = dat_train, method = "forward",nvmax= (ncol(dat_train)-1))
forward_summ = summary(forward_fit)
best.size = which.max(forward_summ$adjr2)
forward_feature = forward_summ$which[best.size,][forward_summ$which[best.size,] == TRUE][-1]
forward_feature = names(forward_feature)
length(forward_feature)
forward_feature = forward_summ$which[best.size,][forward_summ$which[best.size,] == TRUE][-1]
library(leaps)
library(glmnet)
library(caret)
library(randomForest)
#getwd()
#setwd("~/Documents/GitHub/Fall2020-Project3-group6/lib/")
load("../output/feature_train.RData")
# Forward Stepwise Selection-------------------------------------------------------------------------
forward_fit <- regsubsets(label ~ ., data = dat_train, method = "forward",nvmax= (ncol(dat_train)-1))
forward_summ = summary(forward_fit)
best.size = which.max(forward_summ$adjr2)
forward_feature = forward_summ$which[best.size,][forward_summ$which[best.size,] == TRUE][-1]
forward_feature = names(forward_feature)
length(forward_feature)
save(forward_feature, file="../output/forward_feature.RData")
load("../output/forward_feature.RData")
forward_feature
select_train <- dat_train[,forward_feature]
dim(select_train)
if(run.xgboost){
source("../lib/test_xgboost.R")
load("../output/model_xgb.RData")
load("../output/feature_test.RData")
test_result_xgb = xgb_test(model_xgb, dat_test)
pred_xgb = test_result_xgb[[1]]
pred_xgb = ifelse(pred_xgb>0.5, 1, 0)
test_time_xgb = test_result_xgb[[2]]
}
## reweight the test data to represent a balanced label distribution
if(run.xgboost){
accuracy_xgb = mean(dat_test$label == pred_xgb)
label_test <- as.numeric(dat_test$label)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_xgb, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of xgb model is:", accuracy_xgb*100, "%.\n")
cat("The AUC of xgb model is:", auc, ".\n")
}
if(run.xgboost){
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_xgb[1], "s \n")
cat("Time for testing model=", test_time_xgb[1], "s \n")
}
train_dir <- "~/Documents/Columbia/2020Fall/Applied Data Science/Project 3/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
source("../lib/train_xgboost.R")
source("../lib/test_xgboost.R")
source("../lib/cv_xgboost.R")
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
e <-c(0.05, 0.1, 0.2, 0.3, 0.4, 0.5) # best: 0.1
md <- c(3, 6, 8, 10, 15) # best: 6
w <- c(1,3,5,10) # best: 1
c <- c(0.2,0.5,0.8,1) # best: 0.8
if(run.cv){
res_cv <- matrix(0, nrow = length(w)*length(c), ncol = 4)
t=0
for(i in 1:length(w)){
for (j in 1:length(c)){
t=t+1
cat("min_child_weight = ", w[i], ", colsample_bytree = ", c[j], "\n")
res_cv[t,] <- cv_xgb(dat_train, K, para=list(eta=0.1, max_depth=6,
min_child_weight=w[i], colsample_bytree=c[j]))
}
#save(res_cv, file="../output/res_cv.RData")
}
res_cv <- as.data.frame(res_cv)
colnames(res_cv) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv
which.min(res_cv$mean_error)
}
run.cv <- FALSE # run cross-validation on the training set
sample.reweight <- FALSE # run sample reweighting in model training
K <- 5  # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test <- TRUE # run evaluation on an independent test set
run.feature.test <- TRUE # process features for test set
run.baseline <- FALSE
run.xgboost <- TRUE
source("../lib/cv_xgboost.R")
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
e <-c(0.05, 0.1, 0.2, 0.3, 0.4, 0.5) # best: 0.1
md <- c(3, 6, 8, 10, 15) # best: 6
w <- c(1,3,5,10) # best: 1
c <- c(0.2,0.5,0.8,1) # best: 0.8
if(run.cv){
res_cv <- matrix(0, nrow = length(w)*length(c), ncol = 4)
t=0
for(i in 1:length(w)){
for (j in 1:length(c)){
t=t+1
cat("min_child_weight = ", w[i], ", colsample_bytree = ", c[j], "\n")
res_cv[t,] <- cv_xgb(dat_train, K, para=list(eta=0.1, max_depth=6,
min_child_weight=w[i], colsample_bytree=c[j]))
}
#save(res_cv, file="../output/res_cv.RData")
}
res_cv <- as.data.frame(res_cv)
colnames(res_cv) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
res_cv
which.min(res_cv$mean_error)
}
para <- list(objective="binary:logistic", eta=0.1, max_depth=6, min_child_weight=1,
colsample_bytree=0.8, scale_pos_weight=4)
if(run.xgboost){
train_result_xgb <- xgb_train(dat_train, para)
model_xgb <- train_result_xgb[[1]]
train_time_xgb <- train_result_xgb[[2]]
save(model_xgb, file="../output/model_xgb.RData")
save(train_time_xgb, file="../output/train_time_xgb.RData")
}
if(run.xgboost){
source("../lib/test_xgboost.R")
load("../output/model_xgb.RData")
load("../output/feature_test.RData")
test_result_xgb = xgb_test(model_xgb, dat_test)
pred_xgb = test_result_xgb[[1]]
pred_xgb = ifelse(pred_xgb>0.5, 1, 0)
test_time_xgb = test_result_xgb[[2]]
}
## reweight the test data to represent a balanced label distribution
if(run.xgboost){
accuracy_xgb = mean(dat_test$label == pred_xgb)
label_test <- as.numeric(dat_test$label)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_xgb, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of xgb model is:", accuracy_xgb*100, "%.\n")
cat("The AUC of xgb model is:", auc, ".\n")
}
if(run.xgboost){
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_xgb[1], "s \n")
cat("Time for testing model=", test_time_xgb[1], "s \n")
}
train_dir <- "~/Documents/Columbia/2020Fall/Applied Data Science/Project 3/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
run.cv <- FALSE # run cross-validation on the training set
sample.reweight <- FALSE # run sample reweighting in model training
K <- 5  # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test <- TRUE # run evaluation on an independent test set
run.feature.test <- TRUE # process features for test set
run.baseline <- FALSE
run.svm <- TRUE
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
n_files <- length(list.files(train_image_dir))
image_list <- list()
for(i in 1:100){
image_list[[i]] <- readImage(paste0(train_image_dir, sprintf("%04d", i), ".jpg"))
}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
readMat.matrix <- function(index){
return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}
#load fiducial points
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
save(dat_train, file="../output/feature_train.RData")
}else{
load(file="../output/feature_train.RData")
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
save(dat_train, file="../output/feature_train.RData")
}else{
load(file="../output/feature_train.RData")
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
save(dat_test, file="../output/feature_test.RData")
}else{
load(file="../output/feature_test.RData")
}
source("../lib/train_svm.R")
source("../lib/test_svm.R")
run.cv <- FALSE # run cross-validation on the training set
sample.reweight <- FALSE # run sample reweighting in model training
K <- 5  # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test <- TRUE # run evaluation on an independent test set
run.feature.test <- TRUE # process features for test set
run.baseline <- FALSE
run.tune.svm <- FALSE
run.svm <- TRUE
source("../lib/train_svm.R")
source("../lib/test_svm.R")
# try different parameters
if(run.tune.svm){
tune_svm <- tune.svm(label~., data=dat_train, kernel='polynomial', degree=3, cost=10^(0:2), gamma=c(0.003,0.005,0.007), class.weights=list(c('0'=1, '1'=4)), tunecontrol=tune.control(cross = 5))
summary(tune_svm)
}
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.tune.svm){
tune_svm <- tune.svm(label~., data=dat_train, kernel='polynomial', degree=3, cost=10^(0:2), gamma=c(0.003,0.005,0.007), class.weights=list(c('0'=1, '1'=4)), tunecontrol=tune.control(cross = 5))
summary(tune_svm)
}
if(run.svm){
train_result_svm <- svm_train(dat_train, kernel='polynomial', degree=3, cost=10, gamma=0.005, class.weights=c('0'=1, '1'=4))
model_svm <- train_result_svm[[1]]
train_time_svm <- train_result_svm[[2]]
save(model_svm, file="../output/model_svm.RData")
save(train_time_svm, file="../output/train_time_xsvm.RData")
}
if(run.svm){
source("../lib/test_svm.R")
load("../output/model_svm.RData")
load("../output/feature_test.RData")
test_result_svm = svm_test(model_svm, dat_test)
pred_svm = test_result_svm[[1]]
test_time_svm = test_result_svm[[2]]
}
if(run.svm){
accuracy_svm = mean(dat_test$label == pred_svm)
label_test <- as.numeric(dat_test$label)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_svm, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of svm model is:", accuracy_svm*100, "%.\n")
cat("The AUC of svm model is:", auc, ".\n")
}
accuracy_svm = mean(dat_test$label == pred_svm)
label_test <- as.numeric(dat_test$label)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_svm, label_test, weight_test)
weight_test
pred_svm
label_test
dat_test$label
accuracy_svm = mean(dat_test$label == pred_svm)
label_test <- dat_test$label
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_svm, label_test, weight_test)
accuracy_svm = mean(dat_test$label == pred_svm)
label_test <- as.numeric(dat_test$label)
label_test
label_test <- as.numeric(dat_test$label)-1
is.numeric(label_test)
is.numeric(pred_svm)
is.numeric(pred_xgb)
pred_svm
as.numeric(pred_svm)
accuracy_svm = mean(dat_test$label == pred_svm)
if(run.svm){
accuracy_svm = mean(dat_test$label == pred_svm)
label_test <- as.numeric(dat_test$label)-1
pred_svm_num <- as.numeric(pred_svm)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_svm_num, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of svm model is:", accuracy_svm*100, "%.\n")
cat("The AUC of svm model is:", auc, ".\n")
}
if(run.svm){
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_svm[1], "s \n")
cat("Time for testing model=", test_time_svm[1], "s \n")
}
if(!require("EBImage")){
install.packages("BiocManager")
BiocManager::install("EBImage")
}
if(!require("R.matlab")){
install.packages("R.matlab")
}
if(!require("readxl")){
install.packages("readxl")
}
if(!require("dplyr")){
install.packages("dplyr")
}
if(!require("readxl")){
install.packages("readxl")
}
if(!require("ggplot2")){
install.packages("ggplot2")
}
if(!require("caret")){
install.packages("caret")
}
if(!require("glmnet")){
install.packages("glmnet")
}
if(!require("WeightedROC")){
install.packages("WeightedROC")
}
if(!require("gbm")){
install.packages("gbm")
}
if(!require("e1071")){
install.packages("e1071")
}
library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(glmnet)
library(WeightedROC)
library(gbm)
library(e1071)
set.seed(2020)
# setwd("~/Project3-FacialEmotionRecognition/doc")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located.
# use relative path for reproducibility
train_dir <- "~/Documents/Columbia/2020Fall/Applied Data Science/Project 3/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
run.cv <- FALSE # run cross-validation on the training set
sample.reweight <- FALSE # run sample reweighting in model training
K <- 5  # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test <- TRUE # run evaluation on an independent test set
run.feature.test <- TRUE # process features for test set
run.baseline <- FALSE
run.tune.svm <- FALSE
run.svm <- TRUE
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)
n_files <- length(list.files(train_image_dir))
image_list <- list()
for(i in 1:100){
image_list[[i]] <- readImage(paste0(train_image_dir, sprintf("%04d", i), ".jpg"))
}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
readMat.matrix <- function(index){
return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}
#load fiducial points
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
save(dat_train, file="../output/feature_train.RData")
}else{
load(file="../output/feature_train.RData")
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
save(dat_test, file="../output/feature_test.RData")
}else{
load(file="../output/feature_test.RData")
}
source("../lib/train_svm.R")
source("../lib/test_svm.R")
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.tune.svm){
tune_svm <- tune.svm(label~., data=dat_train, kernel='polynomial', degree=3, cost=10^(0:2), gamma=c(0.003,0.005,0.007), class.weights=list(c('0'=1, '1'=4)), tunecontrol=tune.control(cross = 5))
summary(tune_svm)
}
if(run.svm){
train_result_svm <- svm_train(dat_train, kernel='polynomial', degree=3, cost=10, gamma=0.005, class.weights=c('0'=1, '1'=4))
model_svm <- train_result_svm[[1]]
train_time_svm <- train_result_svm[[2]]
save(model_svm, file="../output/model_svm.RData")
save(train_time_svm, file="../output/train_time_svm.RData")
}
if(run.svm){
source("../lib/test_svm.R")
load("../output/model_svm.RData")
load("../output/feature_test.RData")
test_result_svm = svm_test(model_svm, dat_test)
pred_svm = test_result_svm[[1]]
test_time_svm = test_result_svm[[2]]
}
if(run.svm){
accuracy_svm = mean(dat_test$label == pred_svm)
label_test <- as.numeric(dat_test$label)-1
pred_svm_num <- as.numeric(pred_svm)-1
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
weight_test[label_test == v] = 0.5 * length(label_test) / length(label_test[label_test == v])
}
tpr.fpr <- WeightedROC(pred_svm_num, label_test, weight_test)
auc <- WeightedAUC(tpr.fpr)
cat("The accuracy of svm model is:", accuracy_svm*100, "%.\n")
cat("The AUC of svm model is:", auc, ".\n")
}
if(run.svm){
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train_time_svm[1], "s \n")
cat("Time for testing model=", test_time_svm[1], "s \n")
}
